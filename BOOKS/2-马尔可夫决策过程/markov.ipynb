{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 收获和价值的计算(马尔可夫奖励过程)\n",
    "- 收获(return) 收获是对应于状态序列中的某一时刻的 状态的，计算从该状态开始直至结束还能获得的累积奖励 \n",
    "$$G _ { t } = R _ { t + 1 } + \\gamma R _ { t + 2 } + \\ldots = \\sum _ { k = 0 } ^ { \\infty } \\gamma ^ { k } R _ { t + k + 1 }$$\n",
    "- 价值(value) 是马尔科夫奖励过程中状态收获的期望\n",
    "$$ v ( s ) = E [ G _ { t } | S _ { t } = s ] $$\n",
    "- 价值函数展开一窥奥秘\n",
    "$$  \\left. \\begin{array}{l}{ v ( s ) } \\\\ { = E [ G _ { t } | S _ { t } = s ] }\\\\{ = E [ R _ { t + 1 } + \\gamma v ( S _ { t + 1 } ) | S _ { t } = s ] } \\\\ { = R_s + \\gamma v( s ^ { \\prime } ) }\\end{array} \\right.$$\n",
    "- 价值函数=>(贝尔曼方程) 等于此时刻的奖励值和下一个状态的价值。下一个状态的价值就是透过此时刻到下一时刻全部状态的平均价值\n",
    "$$ v ( s ) = R _ { s } + \\gamma \\sum _ { s ^ { \\prime } \\in S } P _ { s s ^ { \\prime } } v ( s ^ { \\prime } ) $$\n",
    "- 用矩阵形式计算价值。可以直接求解。复杂度O(n^3)\n",
    "$$ \\left. \\begin{array}{l}{ v = R + \\gamma P v }\\\\{ ( 1 - \\gamma P ) v = R }\\\\{ v = ( 1 - \\gamma P ) ^ { - 1 } R }\\end{array} \\right. $$\n",
    "> 相较于值计算Return， Return只考虑了某些特殊的情况，并没有从必然性来考虑。所以求他的期望是比较准确的\n",
    "> 这种马尔可夫奖励过程不涉及选择动作，预定了的概率存在，我们只是计算出了每种状态的价值->改进为马尔可夫决策过程(MDP)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\n",
    "# 这些状态有:第 一节课(C1)、第二节课(C2)、第三节课(C3)、泡吧中(Pub)、通过考试(Pass)、浏览手机 (FB)、以及休息退出(Sleep)共 7 个状态\n",
    "# 索引到状态名的对应\n",
    "\n",
    "i_to_n = {}\n",
    "i_to_n[\"0\"] = \"C1\"\n",
    "i_to_n[\"1\"] = \"C2\"\n",
    "i_to_n[\"2\"] = \"C3\"\n",
    "i_to_n[\"3\"] = \"Pass\"\n",
    "i_to_n[\"4\"] = \"Pub\"\n",
    "i_to_n[\"5\"] = \"FB\"\n",
    "i_to_n[\"6\"] = \"Sleep\"\n",
    "\n",
    "n_to_i={}# 状态名到索引的字典\n",
    "for i, name in zip(i_to_n.keys(), i_to_n.values()):\n",
    "    n_to_i[name] = int(i)\n",
    "\n",
    "# 此时我们预定有概率转移矩阵的，因为我们的目标仅仅是计算收获Return和价值Value\n",
    "Pss=[# 状态转移概率矩阵\n",
    "    [ 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0 ],\n",
    "    [ 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.2 ],\n",
    "    [ 0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0 ],\n",
    "    [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0 ],\n",
    "    [ 0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0 ],\n",
    "    [ 0.1, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0 ],\n",
    "    [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0 ]\n",
    "]\n",
    "\n",
    "Pss = np.array(Pss)\n",
    "# 奖励值、对应的是状态。也就是每个状态给他一个奖励值\n",
    "rewards = [-2, -2, -2, 10, 1, -1 ,0]\n",
    "gamma = 0.5\n",
    "\n",
    "# 计算某一条链的累积奖励值Return\n",
    "def compute_return(start_index = 0, chain = None, gamma = 0.5):\n",
    "    retrn, power, gamma = 0.0, 0, gamma\n",
    "    for i in range(start_index, len(chain)):\n",
    "        retrn += np.power(gamma, power) * rewards[n_to_i[chain[i]]]\n",
    "        power += 1\n",
    "    return retrn\n",
    "\n",
    "# 使用矩阵计算价值Value\n",
    "# 相较于值计算Return， Return只考虑了某些特殊的情况，并没有从必然性来考虑。所以求他的期望是比较准确的\n",
    "# 这种马尔可夫奖励过程不涉及选择动作，预定了的概率存在，我们只是计算出了每种状态的价值->改进为马尔可夫决策过程(MDP)\n",
    "def compute_value(Pss, rewards, gamma = 0.999):\n",
    "    # 将rewards转为numpy数组并修改为列向量的形式\n",
    "    rewards = np.array(rewards).reshape(-1, 1)\n",
    "    # np.eye(7,7)为 单 位 矩 阵，inv方 法 为 求 矩 阵 的 逆\n",
    "    values = np.dot(np.linalg.inv(np.eye(7, 7) - gamma * Pss), rewards)\n",
    "    return values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "chains =[\n",
    "[\"C1\", \"C2\", \"C3\", \"Pass\", \"Sleep\"],\n",
    "[\"C1\", \"FB\", \"FB\", \"C1\", \"C2\", \"Sleep\"],\n",
    "[\"C1\", \"C2\", \"C3\", \"Pub\", \"C2\", \"C3\", \"Pass\", \"Sleep\"], [\"C1\", \"FB\", \"FB\", \"C1\", \"C2\", \"C3\", \"Pub\", \"C1\", \"FB\",\\\n",
    "\"FB\", \"FB\", \"C1\", \"C2\", \"C3\", \"Pub\", \"C2\", \"Sleep\"] ]\n",
    "\n",
    "compute_return(0, chains[3], gamma=0.5)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-3.196044921875"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "compute_value(Pss, rewards, gamma=0.99999)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-12.54073351],\n",
       "       [  1.45690179],\n",
       "       [  4.32117045],\n",
       "       [ 10.        ],\n",
       "       [  0.80308417],\n",
       "       [-22.53857963],\n",
       "       [  0.        ]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 马尔可夫决策过程\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 状态数变 成了 5 个，为了方便理解，我们把这五个状态分别命名为:‘浏览手机中’,‘第一节课’,‘第二 节课’,‘第三节课’,‘休息中’;\n",
    "# 行为总数也是 5 个，但具体到某一状态则只有 2 个可能的行 为，这 5 个行为分别命名为:‘浏览手机’,‘学习’,‘离开浏览’,‘泡吧’,‘退出学习’\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('ML': conda)"
  },
  "interpreter": {
   "hash": "0a464861ae92d8053d3f8d92295f3c5faa5c009edc2acd3d573e83a189479915"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}