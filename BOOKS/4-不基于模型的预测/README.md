# 不基于模型的预测

> 策略评估：也就是策略固定、我们计算出他们的价值来

## 蒙特卡罗强化学习(Monte-Carlo reinforcement learning, MC 学习)

> 不清楚 MDP 状态转移概率的情况下,直接从经历完整的状态序列 (episode) 来估计状态的真实价值，并认为某状态的价值等于在多个状态序列中以该状态算得到的所有收获的平均。

特点:
- 基于完整采样
- 不依赖状态转移概率
- 使用的思想就是用平均收获值代替价值。理论上完整的状态序列越多，结果越准确。

出现同一状态在完整序列中多次出现时：
- 首次访问 (first visit) 仅把状态序列中第一次出现该状 态时的收获值纳入到收获平均值的计算中;
- 每次访问 (every visit) 每次出现的该状态，都 计算对应的收获值并纳入到收获平均值的计算中

## 时序差分强化学习(temporal-difference reinforcement learning, TD 学习)

特点：
- 从采样得到的 不完整的状态序列学习，该方法通过合理的引导(bootstrapping)，先估计某状态在该状态序列 完整后可能得到的收获，并在此基础上利用前文所属的累进更新平均值的方法得到该状态的价 值，再通过不断的采样来持续更新这个价值。
- 用的是离开该状态的即刻奖励 Rt+1 与下一时刻状态 St+1 的预估状态价值乘以衰减系数 γ 组成:

公式：
$$ V ( S _ { t } ) \leftarrow V ( S _ { t } ) + \alpha ( R _ { t + 1 } + \gamma V ( S _ { t + 1 } ) - V ( S _ { t } ) ) $$
> 目标值：$R _ { t + 1 } + \gamma V ( S _ { t + 1 } )$

> TD误差 $R _ { t + 1 } + \gamma V ( S _ { t + 1 } ) - V ( S _ { t } )$

## 对比这两种方法
### 相同点
- 它们都不再需要清楚某一状态的所有可能的后续状态以及对应的状态转移概率，因此也不再像动态规划算法那样进行全宽度的回溯更新状 态的价值。
- 通过个体与环境实际交互生成的一系列状态序列来更新 状态的价值。这在解决大规模问题或者不清楚环境动力学特征的问题时十分有效

### 不同点
- MC是通过对整条序列的回放来更新
- TD是根据每步的前行来更新 更快速灵活的更新状态的价值估计
- TD更能考虑一些突发情况，并且把这些情况更新进价值中
> 假如有一次你在驾车回家的路上突然碰到险情: 对面开过来一辆车感觉要和你迎面相撞，严重的话甚至会威胁生命，不过由于最后双方驾驶员都 采取了紧急措施没有让险情实际发生，最后平安到家。如果是使用蒙特卡罗学习，路上发生的这 一险情可能引发的极大负值奖励将不会被考虑，你不会更新在碰到此类险情时的状态的价值;但 是在 TD 学习时，碰到这样的险情过后，你会立即大幅调低这个状态的价值，并在今后再次碰到 类似情况时采取其它行为，例如降低速度等来让自身处在一个价值较高的状态中，尽可能避免发 生意外事件的发生。
- MC学习到的是实际价值的无偏估计
- TD学习到的是实际价值的有偏估计
- MC能适用范围不限于具有马尔科夫性的环境。
- TD使用了 MDP 问题的马儿可夫属性，在具有马尔科夫性的环境下更有效;

### 对比MC、TD、DP
> 本章阐述的蒙特卡罗 (MC) 学习算法、时序差分 (TD) 学习算法和上一章讲述的动态规划
(DP) 算法都可以用来计算状态价值。他们它们的特点也是十分鲜明的，前两种是在不依赖模型 的情况下的常用方法，这其中又以 MC 学习需要完整的状态序列来更新状态价值，TD 学习则不 需要完整的状态序列;DP 算法则是基于模型的计算状态价值的方法，它通过计算一个状态 S 所 有可能的转移状态 S’ 及其转移概率以及对应的即时奖励来计算这个状态 S 的价值。
在是否使用引导数据上，MC 学习并不使用引导数据，它使用实际产生的奖励值来计算状态 价值;TD 和 DP 则都是用后续状态的预估价值作为引导数据来计算当前状态的价值。
在是否采样的问题上，MC 和 TD 不依赖模型，使用的都是个体与环境实际交互产生的采样 状态序列来计算状态价值的，而 DP 则依赖状态转移概率矩阵和奖励函数，全宽度计算状态价 值，没有采样之说。

## n 步时序差分学习简介
